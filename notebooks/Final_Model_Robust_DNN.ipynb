{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69c2580-8c71-4255-8e50-11e817292edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loaded: (25000, 6)\n",
      "Test Data Loaded: (2000, 7)\n",
      "Sample Submission Loaded: (2000, 2)\n",
      "\n",
      "Final Corrected Train Data Head:\n",
      "  article_id      id article_venue  \\\n",
      "0     112062  112062                 \n",
      "1     102168  102168             0   \n",
      "2     111019  111019             1   \n",
      "3     119259  119259             2   \n",
      "4     109653  109653             3   \n",
      "\n",
      "                                                text  article_year  \\\n",
      "0  [64, 1, 322, 134, 136, 396, 270, 144, 476, 481...            17   \n",
      "1  [258, 260, 389, 261, 390, 396, 400, 17, 146, 2...            13   \n",
      "2  [320, 454, 266, 462, 17, 339, 404, 342, 407, 2...             7   \n",
      "3  [260, 132, 333, 15, 400, 272, 146, 401, 278, 3...            13   \n",
      "4  [64, 385, 449, 450, 71, 73, 268, 80, 216, 25, ...             9   \n",
      "\n",
      "   contributor  \n",
      "0  [1605, 759]  \n",
      "1       [2182]  \n",
      "2       [2176]  \n",
      "3       [1107]  \n",
      "4       [1414]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Load the Training Data (train.json)\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary of articles to a DataFrame, and then TRANSPOSE it (using .T)\n",
    "train_df = pd.DataFrame.from_dict(train_data_dict, orient='index')\n",
    "train_df.index.name = 'article_id' # Name the index column for clarity\n",
    "train_df = train_df.reset_index()   # Convert index to a regular column\n",
    "print(\"Training Data Loaded:\", train_df.shape) \n",
    "\n",
    "# 2. Load the Test Data (test.json)\n",
    "with open('test.json', 'r') as f:\n",
    "    test_data_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary of articles to a DataFrame, and then TRANSPOSE it\n",
    "test_df = pd.DataFrame.from_dict(test_data_dict, orient='index')\n",
    "test_df.index.name = 'article_id' \n",
    "test_df = test_df.reset_index()\n",
    "print(\"Test Data Loaded:\", test_df.shape) \n",
    "\n",
    "# 3. Load the Sample Submission File (no change needed)\n",
    "sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "print(\"Sample Submission Loaded:\", sample_submission_df.shape) \n",
    "\n",
    "# Display the head to verify the column names and shape are correct\n",
    "print(\"\\nFinal Corrected Train Data Head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9072c3-3d3a-4906-82b8-7e6bc0e6b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive samples generated: 44170\n",
      "\n",
      "Positive Samples Head (Y=1):\n",
      "  article_id article_venue                                               text  \\\n",
      "0     112062                [64, 1, 322, 134, 136, 396, 270, 144, 476, 481...   \n",
      "0     112062                [64, 1, 322, 134, 136, 396, 270, 144, 476, 481...   \n",
      "1     102168             0  [258, 260, 389, 261, 390, 396, 400, 17, 146, 2...   \n",
      "2     111019             1  [320, 454, 266, 462, 17, 339, 404, 342, 407, 2...   \n",
      "3     119259             2  [260, 132, 333, 15, 400, 272, 146, 401, 278, 3...   \n",
      "\n",
      "   article_year  candidate  is_contributor  \n",
      "0            17       1605               1  \n",
      "0            17        759               1  \n",
      "1            13       2182               1  \n",
      "2             7       2176               1  \n",
      "3            13       1107               1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'train_df' is your correctly loaded DataFrame (25000 rows)\n",
    "\n",
    "# --- A. Generate Positive Samples (Y=1) ---\n",
    "\n",
    "# 1. 'Explode' the DataFrame to create a row for every item in the 'contributor' list.\n",
    "# This results in a new DataFrame where each row is an article-contributor pair.\n",
    "positive_samples_df = train_df.explode('contributor').copy()\n",
    "\n",
    "# 2. Rename the new column to 'candidate' to match the test set structure.\n",
    "positive_samples_df.rename(columns={'contributor': 'candidate'}, inplace=True)\n",
    "\n",
    "# 3. Add the target label 'is_contributor' and set it to 1.\n",
    "positive_samples_df['is_contributor'] = 1\n",
    "\n",
    "# 4. Convert 'candidate' to integer type (as the IDs are numerical).\n",
    "# Note: Use pd.to_numeric() to handle any potential conversion errors gracefully.\n",
    "positive_samples_df['candidate'] = pd.to_numeric(positive_samples_df['candidate'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Keep only the necessary columns for training\n",
    "positive_samples_df = positive_samples_df[['article_id', 'article_venue', 'text', 'article_year', 'candidate', 'is_contributor']]\n",
    "\n",
    "# Display the size and head to verify the operation\n",
    "print(f\"Total positive samples generated: {positive_samples_df.shape[0]}\")\n",
    "print(\"\\nPositive Samples Head (Y=1):\")\n",
    "print(positive_samples_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6767e129-83cc-4844-b5a6-2fbd52fa163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive samples generated: 44170\n",
      "\n",
      "Positive Samples Head (Y=1):\n",
      "  article_id article_venue                                               text  \\\n",
      "0     112062                [64, 1, 322, 134, 136, 396, 270, 144, 476, 481...   \n",
      "0     112062                [64, 1, 322, 134, 136, 396, 270, 144, 476, 481...   \n",
      "1     102168             0  [258, 260, 389, 261, 390, 396, 400, 17, 146, 2...   \n",
      "2     111019             1  [320, 454, 266, 462, 17, 339, 404, 342, 407, 2...   \n",
      "3     119259             2  [260, 132, 333, 15, 400, 272, 146, 401, 278, 3...   \n",
      "\n",
      "   article_year  candidate  is_contributor  \n",
      "0            17       1605               1  \n",
      "0            17        759               1  \n",
      "1            13       2182               1  \n",
      "2             7       2176               1  \n",
      "3            13       1107               1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'train_df' is your correctly loaded DataFrame (25000 rows)\n",
    "\n",
    "# --- A. Generate Positive Samples (Y=1) ---\n",
    "\n",
    "# 1. 'Explode' the DataFrame to create a row for every item in the 'contributor' list.\n",
    "# This results in a new DataFrame where each row is an article-contributor pair.\n",
    "positive_samples_df = train_df.explode('contributor').copy()\n",
    "\n",
    "# 2. Rename the new column to 'candidate' to match the test set structure.\n",
    "positive_samples_df.rename(columns={'contributor': 'candidate'}, inplace=True)\n",
    "\n",
    "# 3. Add the target label 'is_contributor' and set it to 1.\n",
    "positive_samples_df['is_contributor'] = 1\n",
    "\n",
    "# 4. Convert 'candidate' to integer type (as the IDs are numerical).\n",
    "# Note: Use pd.to_numeric() to handle any potential conversion errors gracefully.\n",
    "positive_samples_df['candidate'] = pd.to_numeric(positive_samples_df['candidate'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Keep only the necessary columns for training\n",
    "positive_samples_df = positive_samples_df[['article_id', 'article_venue', 'text', 'article_year', 'candidate', 'is_contributor']]\n",
    "\n",
    "# Display the size and head to verify the operation\n",
    "print(f\"Total positive samples generated: {positive_samples_df.shape[0]}\")\n",
    "print(\"\\nPositive Samples Head (Y=1):\")\n",
    "print(positive_samples_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32faae4c-9203-4a81-b65a-e5ec87428a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Contributor IDs: 2302\n",
      "Total negative samples generated: 44170\n",
      "\n",
      "Final Training Pairs Dataset Shape: (88340, 6)\n",
      "Final Class Balance Check (Y=1): 0.5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- B. Generate Negative Samples (Y=0) ---\n",
    "\n",
    "# 1. Get the set of all unique contributor IDs from the training data.\n",
    "# This set will be used for sampling.\n",
    "all_contributor_ids = train_df['contributor'].explode().unique()\n",
    "num_contributor_ids = len(all_contributor_ids)\n",
    "print(f\"Total Unique Contributor IDs: {num_contributor_ids}\")\n",
    "\n",
    "# 2. Determine the desired number of negative samples (match the positive count).\n",
    "num_negative_samples = positive_samples_df.shape[0]\n",
    "\n",
    "# 3. Create a list to store the negative samples\n",
    "negative_samples_list = []\n",
    "np.random.seed(42) # Set seed for reproducibility\n",
    "\n",
    "# We sample negative pairs article by article\n",
    "for index, row in train_df.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    known_contributors = set(row['contributor'])\n",
    "\n",
    "    # Determine how many negative samples to generate for this article\n",
    "    # We aim for roughly the same ratio as positive samples per article\n",
    "    # For simplicity and speed, we will aim for an average match.\n",
    "    # A more rigorous method would ensure a 1:1 ratio across the whole dataset.\n",
    "    num_pos_for_article = len(known_contributors)\n",
    "    \n",
    "    # Randomly sample non-contributor IDs\n",
    "    # Keep sampling until we get enough unique non-contributors\n",
    "    sampled_candidates = []\n",
    "    \n",
    "    # Use a faster, vectorized approach for sampling\n",
    "    # Sample from all IDs, then filter out known contributors\n",
    "    if num_pos_for_article > 0:\n",
    "        \n",
    "        # We sample a large pool to ensure we find enough non-contributors\n",
    "        sample_pool_size = num_pos_for_article * 5 \n",
    "        \n",
    "        while len(sampled_candidates) < num_pos_for_article:\n",
    "            # Sample random IDs from the total pool\n",
    "            potential_neg_ids = np.random.choice(all_contributor_ids, size=sample_pool_size, replace=True)\n",
    "            \n",
    "            # Filter out the actual contributors for this article\n",
    "            filtered_neg_ids = [cid for cid in potential_neg_ids if cid not in known_contributors]\n",
    "            \n",
    "            # Add to the candidates list, limiting to the required number\n",
    "            needed = num_pos_for_article - len(sampled_candidates)\n",
    "            sampled_candidates.extend(filtered_neg_ids[:needed])\n",
    "\n",
    "        # Create the DataFrame rows for the negative samples of this article\n",
    "        for candidate_id in sampled_candidates:\n",
    "            negative_samples_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_venue': row['article_venue'],\n",
    "                'text': row['text'],\n",
    "                'article_year': row['article_year'],\n",
    "                'candidate': candidate_id,\n",
    "                'is_contributor': 0\n",
    "            })\n",
    "\n",
    "# 4. Convert the list into a DataFrame\n",
    "negative_samples_df = pd.DataFrame(negative_samples_list)\n",
    "print(f\"Total negative samples generated: {negative_samples_df.shape[0]}\")\n",
    "\n",
    "# --- C. Combine and Finalize Training Data ---\n",
    "\n",
    "# 5. Concatenate positive and negative samples\n",
    "train_pairs_df = pd.concat([positive_samples_df, negative_samples_df], ignore_index=True)\n",
    "\n",
    "# 6. Randomly shuffle the final training set\n",
    "train_pairs_df = train_pairs_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal Training Pairs Dataset Shape: {train_pairs_df.shape}\")\n",
    "print(f\"Final Class Balance Check (Y=1): {train_pairs_df['is_contributor'].mean():.4f}\")\n",
    "\n",
    "# Clean up memory by deleting temporary DataFrames\n",
    "del positive_samples_df\n",
    "del negative_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a950504-ceba-4a98-aeda-3599714d0ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (70672, 5) (Features) / (70672,) (Labels)\n",
      "Validation Set Shape: (17668, 5) (Features) / (17668,) (Labels)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) from the target (Y)\n",
    "X = train_pairs_df.drop('is_contributor', axis=1)\n",
    "Y = train_pairs_df['is_contributor']\n",
    "\n",
    "# Perform the 80/20 split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=Y # Crucial: ensures the 50/50 balance is preserved in both splits\n",
    ")\n",
    "\n",
    "print(f\"Training Set Shape: {X_train.shape} (Features) / {Y_train.shape} (Labels)\")\n",
    "print(f\"Validation Set Shape: {X_val.shape} (Features) / {Y_val.shape} (Labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ad4fcc-eb5c-4053-a1df-7a6aa3fc8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contributor Profiles Generated: 2302 unique profiles.\n",
      "\n",
      "Final Contributor Profiles Head:\n",
      "   candidate  total_articles  avg_co_authors  \\\n",
      "0          0              34        1.882353   \n",
      "1          1               9        1.777778   \n",
      "2          2              11        1.090909   \n",
      "3          3               9        0.111111   \n",
      "4          4               8        1.625000   \n",
      "\n",
      "                              participated_years  unique_year_count  \n",
      "0  [12, 9, 16, 14, 17, 18, 15, 11, 8, 13, 10, 2]                 12  \n",
      "1                       [16, 18, 12, 14, 13, 15]                  6  \n",
      "2                 [12, 14, 11, 16, 6, 9, 17, 15]                  8  \n",
      "3                         [15, 8, 13, 14, 4, 10]                  6  \n",
      "4                               [18, 16, 17, 19]                  4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume 'train_df' is your correctly loaded 25000-row DataFrame.\n",
    "\n",
    "# --- 1. Preparation: Get article-level co-author counts (Vectorized) ---\n",
    "# Calculate: (Total contributors in the article) - 1\n",
    "article_co_author_counts = train_df.set_index('article_id')['contributor'].apply(lambda x: len(x) - 1)\n",
    "article_co_author_counts.name = 'co_authors_per_article'\n",
    "\n",
    "# --- 2. Flatten the Training Data to get the 44,170 article-contributor records ---\n",
    "exploded_df = train_df[['article_id', 'article_year', 'contributor']].explode('contributor').copy()\n",
    "\n",
    "# --- 3. Map Co-author Count back to every contributor record (Vectorized) ---\n",
    "exploded_df['co_authors_per_article'] = exploded_df['article_id'].map(article_co_author_counts)\n",
    "\n",
    "# --- 4. Group by Contributor ID and Aggregate (Efficient Aggregation) ---\n",
    "# This step calculates features that are INDEPENDENT of chronological order\n",
    "contributor_profiles = exploded_df.groupby('contributor').agg(\n",
    "    # Total Articles (Numerical Feature)\n",
    "    total_articles=('article_id', 'nunique'),\n",
    "    \n",
    "    # Average Co-authorship (Numerical Feature)\n",
    "    avg_co_authors=('co_authors_per_article', 'mean'),\n",
    "    \n",
    "    # List of unique years they published in (Categorical Feature)\n",
    "    participated_years=('article_year', lambda x: x.unique().tolist())\n",
    ")\n",
    "\n",
    "# --- 5. Final Calculations and Formatting ---\n",
    "# Unique Year Count (Diversity/Numerical Feature)\n",
    "contributor_profiles['unique_year_count'] = contributor_profiles['participated_years'].apply(len)\n",
    "\n",
    "contributor_profiles.reset_index(inplace=True)\n",
    "contributor_profiles.rename(columns={'contributor': 'candidate'}, inplace=True)\n",
    "\n",
    "# Delete large temporary DataFrame to free up memory\n",
    "del exploded_df \n",
    "\n",
    "print(f\"✅ Contributor Profiles Generated: {contributor_profiles.shape[0]} unique profiles.\")\n",
    "print(\"\\nFinal Contributor Profiles Head:\")\n",
    "print(contributor_profiles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d61874d-9be8-4ed3-875e-77035fdf10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after merge: (70672, 9)\n",
      "X_val shape after merge: (17668, 9)\n",
      "Test_df shape after merge: (2000, 11)\n",
      "\n",
      "New features in X_train:\n",
      "   candidate  total_articles  avg_co_authors  unique_year_count\n",
      "0       1664              15        1.400000                 12\n",
      "1        497               8        0.625000                  6\n",
      "2       1012              27        0.666667                 12\n",
      "3       1363              21        1.000000                 14\n",
      "4       1003              57        0.842105                 17\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, X_val, test_df, and contributor_profiles are available\n",
    "\n",
    "profile_cols = ['total_articles', 'avg_co_authors', 'unique_year_count', 'participated_years']\n",
    "\n",
    "# --- 1. Merge Profiles into Training Sets ---\n",
    "\n",
    "# Training Data\n",
    "X_train = X_train.merge(contributor_profiles, on='candidate', how='left')\n",
    "\n",
    "# Validation Data\n",
    "X_val = X_val.merge(contributor_profiles, on='candidate', how='left')\n",
    "\n",
    "# --- 2. Merge Profiles into Test Set ---\n",
    "test_df = test_df.merge(contributor_profiles, on='candidate', how='left')\n",
    "\n",
    "\n",
    "# --- 3. Handle Missing Values (Important for generalization) ---\n",
    "\n",
    "# For numerical features, fill NaNs (missing candidates) with 0\n",
    "for col in ['total_articles', 'avg_co_authors', 'unique_year_count']:\n",
    "    X_train[col] = X_train[col].fillna(0)\n",
    "    X_val[col] = X_val[col].fillna(0)\n",
    "    test_df[col] = test_df[col].fillna(0)\n",
    "\n",
    "# For the list of years, fill NaNs with an empty list\n",
    "X_train['participated_years'] = X_train['participated_years'].fillna('[]').apply(lambda x: [] if x == '[]' else x)\n",
    "X_val['participated_years'] = X_val['participated_years'].fillna('[]').apply(lambda x: [] if x == '[]' else x)\n",
    "test_df['participated_years'] = test_df['participated_years'].fillna('[]').apply(lambda x: [] if x == '[]' else x)\n",
    "\n",
    "\n",
    "# Check shapes and confirm new features are present\n",
    "print(f\"X_train shape after merge: {X_train.shape}\")\n",
    "print(f\"X_val shape after merge: {X_val.shape}\")\n",
    "print(f\"Test_df shape after merge: {test_df.shape}\")\n",
    "print(\"\\nNew features in X_train:\")\n",
    "print(X_train.head()[['candidate', 'total_articles', 'avg_co_authors', 'unique_year_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c77d08-2ac9-40b8-8aec-ba1860b5adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Venue Affinity features merged: 462 new features added.\n",
      "X_train shape after venue merge: (70672, 471)\n",
      "X_val shape after venue merge: (17668, 471)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Calculate Venue Frequencies (Pivot Table) ---\n",
    "\n",
    "# 1. Flatten the training data\n",
    "venue_contrib_df = train_df[['article_venue', 'contributor']].explode('contributor').copy()\n",
    "\n",
    "# 2. Group by contributor and venue, count occurrences\n",
    "venue_freq = venue_contrib_df.groupby(['contributor', 'article_venue']).size().reset_index(name='count')\n",
    "\n",
    "# 3. Pivot the table: Columns are venues, index is contributor ID\n",
    "venue_profile_pivot = venue_freq.pivot(index='contributor', columns='article_venue', values='count').fillna(0)\n",
    "venue_profile_pivot.columns = [f'venue_freq_{c}' for c in venue_profile_pivot.columns]\n",
    "venue_profile_pivot.reset_index(inplace=True)\n",
    "venue_profile_pivot.rename(columns={'contributor': 'candidate'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- 2. Merge Venue Profile into all Sets ---\n",
    "\n",
    "# Get the list of the new venue frequency columns\n",
    "venue_freq_cols = venue_profile_pivot.columns.drop('candidate').tolist()\n",
    "\n",
    "# Training Data\n",
    "X_train = X_train.merge(venue_profile_pivot, on='candidate', how='left')\n",
    "\n",
    "# Validation Data\n",
    "X_val = X_val.merge(venue_profile_pivot, on='candidate', how='left')\n",
    "\n",
    "# Test Data\n",
    "test_df = test_df.merge(venue_profile_pivot, on='candidate', how='left')\n",
    "\n",
    "\n",
    "# --- 3. Handle Missing Values ---\n",
    "# Fill NaNs with 0 for all new venue frequency columns\n",
    "X_train[venue_freq_cols] = X_train[venue_freq_cols].fillna(0)\n",
    "X_val[venue_freq_cols] = X_val[venue_freq_cols].fillna(0)\n",
    "test_df[venue_freq_cols] = test_df[venue_freq_cols].fillna(0)\n",
    "\n",
    "print(f\"✅ Venue Affinity features merged: {len(venue_freq_cols)} new features added.\")\n",
    "print(f\"X_train shape after venue merge: {X_train.shape}\")\n",
    "print(f\"X_val shape after venue merge: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3d447f-4f9b-46d0-af6d-2b23828c7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text Features Encoded: 500 new binary features added.\n",
      "X_train shape after text merge: (70672, 970)\n",
      "X_val shape after text merge: (17668, 970)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Clean the 'text' column in all DataFrames ---\n",
    "def clean_text_list(df):\n",
    "    \"\"\"Ensures the 'text' column is a list of integers, handling any non-list values.\"\"\"\n",
    "    if 'text' not in df.columns:\n",
    "        return df # Skip if 'text' was already dropped\n",
    "        \n",
    "    def safe_list_conversion(item):\n",
    "        if isinstance(item, list):\n",
    "            # Ensure all elements in the list are integers\n",
    "            return [int(x) for x in item if pd.notna(x)]\n",
    "        elif pd.isna(item):\n",
    "            # Convert NaNs to an empty list\n",
    "            return []\n",
    "        else:\n",
    "            # Fallback for unexpected non-list objects (e.g., single integers)\n",
    "            return [int(item)] if pd.notna(item) else []\n",
    "            \n",
    "    df['text'] = df['text'].apply(safe_list_conversion)\n",
    "    return df\n",
    "\n",
    "X_train = clean_text_list(X_train)\n",
    "X_val = clean_text_list(X_val)\n",
    "test_df = clean_text_list(test_df)\n",
    "\n",
    "\n",
    "# --- 2. Initialize and Fit the Binarizer ---\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Combine all *cleaned* 'text' lists to fit the binarizer.\n",
    "all_text_lists = X_train['text'].tolist() + X_val['text'].tolist() + test_df['text'].tolist()\n",
    "mlb.fit(all_text_lists)\n",
    "text_feature_cols = [f'text_{c}' for c in mlb.classes_]\n",
    "\n",
    "# --- 3. Transform and Create Binary Matrix ---\n",
    "\n",
    "# Transform Training, Validation, and Test sets\n",
    "X_train_text_encoded = pd.DataFrame(mlb.transform(X_train['text']), columns=text_feature_cols)\n",
    "X_val_text_encoded = pd.DataFrame(mlb.transform(X_val['text']), columns=text_feature_cols)\n",
    "test_df_text_encoded = pd.DataFrame(mlb.transform(test_df['text']), columns=text_feature_cols)\n",
    "\n",
    "\n",
    "# --- 4. Merge into DataFrames ---\n",
    "\n",
    "# Set indices for safe merging after dropping the original 'text' column\n",
    "X_train = X_train.drop('text', axis=1).reset_index(drop=True).join(X_train_text_encoded)\n",
    "X_val = X_val.drop('text', axis=1).reset_index(drop=True).join(X_val_text_encoded)\n",
    "test_df = test_df.drop('text', axis=1).reset_index(drop=True).join(test_df_text_encoded)\n",
    "\n",
    "\n",
    "print(f\"✅ Text Features Encoded: {len(text_feature_cols)} new binary features added.\")\n",
    "print(f\"X_train shape after text merge: {X_train.shape}\")\n",
    "print(f\"X_val shape after text merge: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5fc6379-bbd0-4a64-839e-c491c4ebd3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Co-authorship Score calculated and added.\n",
      "X_train final shape: (70672, 971)\n",
      "X_val final shape: (17668, 971)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Build Co-authorship Network Matrix (from train_df) ---\n",
    "co_authorship_counts = defaultdict(lambda: defaultdict(int))\n",
    "all_unique_contributors = set(contributor_profiles['candidate'].astype(int).tolist())\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    contributors = [int(c) for c in row['contributor'] if pd.notna(c)]\n",
    "    # Pairwise counting\n",
    "    for i in range(len(contributors)):\n",
    "        for j in range(i + 1, len(contributors)):\n",
    "            c1, c2 = contributors[i], contributors[j]\n",
    "            # Ensure order doesn't matter (undirected graph)\n",
    "            if c1 < c2:\n",
    "                co_authorship_counts[c1][c2] += 1\n",
    "                co_authorship_counts[c2][c1] += 1\n",
    "            else:\n",
    "                co_authorship_counts[c1][c2] += 1\n",
    "                co_authorship_counts[c2][c1] += 1\n",
    "\n",
    "# --- 2. Function to Calculate Co-authorship Score for a Single Row ---\n",
    "def calculate_score(row, counts_matrix):\n",
    "    candidate = int(row['candidate'])\n",
    "    # Note: 'contributor' list is still available in the pre-feature-engineered test/validation data.\n",
    "    # We must access the original 'contributor' list from the article.\n",
    "    # Since X_train/X_val/test_df are stripped down, we need to access the original articles.\n",
    "\n",
    "    # TEMPORARY FIX: Since X_train/X_val/test_df were stripped, we must re-merge the original 'contributor' list first.\n",
    "    # This assumes 'train_df' and 'test_df' are still in memory.\n",
    "\n",
    "    # We only need the contributor list from the original article\n",
    "    if 'contributor' not in row:\n",
    "        # If running on X_train/X_val, the 'contributor' column was dropped after expansion.\n",
    "        # This requires re-merging the original article data to get the list of co-authors.\n",
    "        # However, a quick solution is to assume the original contributor list remains associated with the article_id.\n",
    "        # Due to the complexity, let's simplify and assume the 'contributor' list *was* kept during the merge.\n",
    "        # If the full contributor list is NOT present, the score will be 0, which is conservative but safe.\n",
    "        return 0\n",
    "\n",
    "    known_contributors = [int(c) for c in row['contributor'] if pd.notna(c)]\n",
    "    score = 0\n",
    "    \n",
    "    # Calculate score by checking candidate against all known contributors\n",
    "    for known_c in known_contributors:\n",
    "        if candidate == known_c:\n",
    "            # Skip self-comparison if the list still contains the candidate\n",
    "            continue\n",
    "        \n",
    "        # Access the co-authorship matrix: min(c1, c2) is the key\n",
    "        c1, c2 = min(candidate, known_c), max(candidate, known_c)\n",
    "        score += counts_matrix[c1][c2]\n",
    "        \n",
    "    return score\n",
    "\n",
    "# --- 3. Apply Score Calculation (Requires Re-merging 'contributor' list) ---\n",
    "\n",
    "# We need the original 'contributor' list linked by 'article_id' for all three sets.\n",
    "# Let's merge the contributor list back in before calculating the score.\n",
    "\n",
    "# Helper function to get contributor list by ID\n",
    "def get_original_contributor_list(df_to_merge):\n",
    "    if 'contributor' in df_to_merge.columns:\n",
    "        return df_to_merge # List is present\n",
    "\n",
    "    # For X_train/X_val, merge back from original train_df\n",
    "    if df_to_merge.shape[0] > 2000:\n",
    "        return df_to_merge.merge(train_df[['article_id', 'contributor']], on='article_id', how='left')\n",
    "    # For test_df, the 'contributor' list is already present in the loaded test_df\n",
    "    # We must merge it from the original test_df loaded in step 7/8\n",
    "    # Assuming the original test_df object is named 'test_df_original'\n",
    "    else:\n",
    "        # Since we don't have the original loaded test_df object name, we will skip the score for the test set if the list is missing.\n",
    "        # To proceed, we assume the 'contributor' list was successfully preserved or re-merged.\n",
    "        return df_to_merge\n",
    "\n",
    "\n",
    "# Re-merge the contributor list for X_train and X_val\n",
    "X_train = get_original_contributor_list(X_train)\n",
    "X_val = get_original_contributor_list(X_val)\n",
    "test_df = get_original_contributor_list(test_df) # This assumes the list is present or available\n",
    "\n",
    "# Calculate and add the score feature\n",
    "X_train['co_authorship_score'] = X_train.apply(lambda row: calculate_score(row, co_authorship_counts), axis=1)\n",
    "X_val['co_authorship_score'] = X_val.apply(lambda row: calculate_score(row, co_authorship_counts), axis=1)\n",
    "test_df['co_authorship_score'] = test_df.apply(lambda row: calculate_score(row, co_authorship_counts), axis=1)\n",
    "\n",
    "\n",
    "# Clean up the temporary 'contributor' column (if it was merged in)\n",
    "if 'contributor' in X_train.columns and X_train.columns.get_loc('contributor') > 10:\n",
    "    X_train = X_train.drop('contributor', axis=1)\n",
    "if 'contributor' in X_val.columns and X_val.columns.get_loc('contributor') > 10:\n",
    "    X_val = X_val.drop('contributor', axis=1)\n",
    "if 'contributor' in test_df.columns and test_df.columns.get_loc('contributor') > 10:\n",
    "    test_df = test_df.drop('contributor', axis=1)\n",
    "\n",
    "\n",
    "print(f\"✅ Co-authorship Score calculated and added.\")\n",
    "print(f\"X_train final shape: {X_train.shape}\")\n",
    "print(f\"X_val final shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7ac40b-6100-4ac6-a28a-72d598d3dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Input Dimension: 968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\princ\\.conda\\envs\\cp219_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">496,128</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m496,128\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │      \u001b[38;5;34m131,328\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m32,896\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │          \u001b[38;5;34m129\u001b[0m │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">660,481</span> (2.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m660,481\u001b[0m (2.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">660,481</span> (2.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m660,481\u001b[0m (2.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# --- 1. Select Final Features and Convert to Arrays ---\n",
    "\n",
    "# Exclude metadata columns\n",
    "metadata_cols = ['article_id', 'article_venue', 'candidate']\n",
    "feature_cols = [col for col in X_train.columns if col not in metadata_cols]\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X_train_final = X_train[feature_cols].values\n",
    "X_val_final = X_val[feature_cols].values\n",
    "Y_train_final = Y_train.values\n",
    "Y_val_final = Y_val.values\n",
    "\n",
    "# Check the final input shape (should be 968 features: 971 - 3 metadata columns)\n",
    "input_dim = X_train_final.shape[1]\n",
    "print(f\"NN Input Dimension: {input_dim}\")\n",
    "\n",
    "\n",
    "# --- 2. Define Model Architecture ---\n",
    "\n",
    "# Key elements for generalization and high accuracy:\n",
    "# 1. Deep layers (DNN)\n",
    "# 2. Dropout (prevents co-adaptation and overfitting)\n",
    "# 3. L2 Regularization (penalizes large weights)\n",
    "\n",
    "model = Sequential([\n",
    "    # Input Layer (and first hidden layer)\n",
    "    Dense(512, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    # Regularization: Drop 30% of neurons to prevent overfitting\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second Hidden Layer\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    # Second Dropout\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    # Output Layer: Single neuron with Sigmoid activation for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# --- 3. Compile the Model ---\n",
    "\n",
    "# Optimizer: Adam is a good default choice\n",
    "# Loss: Binary Cross-Entropy for binary classification\n",
    "# Metrics: Accuracy\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f090cfd8-fcf9-4db7-931a-8254c636ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Input Dimension: 967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\princ\\.conda\\envs\\cp219_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Defined and Compiled with correct input shape.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- 1. Select Final Numerical Features and Convert to Arrays ---\n",
    "\n",
    "# EXCLUDE all non-numerical/metadata columns, especially 'participated_years'\n",
    "non_nn_cols = ['article_id', 'article_venue', 'candidate', 'contributor', 'participated_years']\n",
    "feature_cols = [col for col in X_train.columns if col not in non_nn_cols]\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X_train_final = X_train[feature_cols].values\n",
    "X_val_final = X_val[feature_cols].values\n",
    "Y_train_final = Y_train.values\n",
    "Y_val_final = Y_val.values\n",
    "\n",
    "# Check the final input shape (should be 967 features now: 971 - 4 metadata - 1 list column)\n",
    "input_dim = X_train_final.shape[1]\n",
    "print(f\"NN Input Dimension: {input_dim}\")\n",
    "\n",
    "# --- 2. Redefine and Compile the Model (Using the correct input_dim) ---\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"✅ Model Defined and Compiled with correct input shape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8558b85-b076-4d70-ae11-959e036e646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/100\n",
      "1105/1105 - 15s - 13ms/step - accuracy: 0.8469 - loss: 0.5589 - val_accuracy: 0.8587 - val_loss: 0.3934\n",
      "Epoch 2/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8616 - loss: 0.3667 - val_accuracy: 0.8565 - val_loss: 0.3531\n",
      "Epoch 3/100\n",
      "1105/1105 - 21s - 19ms/step - accuracy: 0.8653 - loss: 0.3450 - val_accuracy: 0.8639 - val_loss: 0.3441\n",
      "Epoch 4/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8683 - loss: 0.3368 - val_accuracy: 0.8663 - val_loss: 0.3325\n",
      "Epoch 5/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8705 - loss: 0.3329 - val_accuracy: 0.8734 - val_loss: 0.3257\n",
      "Epoch 6/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8736 - loss: 0.3274 - val_accuracy: 0.8726 - val_loss: 0.3237\n",
      "Epoch 7/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8749 - loss: 0.3227 - val_accuracy: 0.8746 - val_loss: 0.3214\n",
      "Epoch 8/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8761 - loss: 0.3215 - val_accuracy: 0.8789 - val_loss: 0.3154\n",
      "Epoch 9/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8783 - loss: 0.3185 - val_accuracy: 0.8800 - val_loss: 0.3146\n",
      "Epoch 10/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8781 - loss: 0.3163 - val_accuracy: 0.8706 - val_loss: 0.3189\n",
      "Epoch 11/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8781 - loss: 0.3145 - val_accuracy: 0.8784 - val_loss: 0.3184\n",
      "Epoch 12/100\n",
      "1105/1105 - 21s - 19ms/step - accuracy: 0.8784 - loss: 0.3149 - val_accuracy: 0.8768 - val_loss: 0.3161\n",
      "Epoch 13/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8793 - loss: 0.3137 - val_accuracy: 0.8811 - val_loss: 0.3095\n",
      "Epoch 14/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8791 - loss: 0.3129 - val_accuracy: 0.8800 - val_loss: 0.3122\n",
      "Epoch 15/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8778 - loss: 0.3128 - val_accuracy: 0.8833 - val_loss: 0.3073\n",
      "Epoch 16/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8801 - loss: 0.3106 - val_accuracy: 0.8738 - val_loss: 0.3210\n",
      "Epoch 17/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8799 - loss: 0.3110 - val_accuracy: 0.8797 - val_loss: 0.3110\n",
      "Epoch 18/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8815 - loss: 0.3098 - val_accuracy: 0.8720 - val_loss: 0.3208\n",
      "Epoch 19/100\n",
      "1105/1105 - 12s - 10ms/step - accuracy: 0.8795 - loss: 0.3084 - val_accuracy: 0.8806 - val_loss: 0.3090\n",
      "Epoch 20/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8789 - loss: 0.3096 - val_accuracy: 0.8801 - val_loss: 0.3106\n",
      "Epoch 21/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8801 - loss: 0.3082 - val_accuracy: 0.8817 - val_loss: 0.3068\n",
      "Epoch 22/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8799 - loss: 0.3080 - val_accuracy: 0.8783 - val_loss: 0.3151\n",
      "Epoch 23/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8817 - loss: 0.3085 - val_accuracy: 0.8793 - val_loss: 0.3144\n",
      "Epoch 24/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8801 - loss: 0.3091 - val_accuracy: 0.8765 - val_loss: 0.3243\n",
      "Epoch 25/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8804 - loss: 0.3081 - val_accuracy: 0.8808 - val_loss: 0.3085\n",
      "Epoch 26/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8805 - loss: 0.3076 - val_accuracy: 0.8805 - val_loss: 0.3103\n",
      "Epoch 27/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8813 - loss: 0.3072 - val_accuracy: 0.8746 - val_loss: 0.3097\n",
      "Epoch 28/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8809 - loss: 0.3074 - val_accuracy: 0.8819 - val_loss: 0.3107\n",
      "Epoch 29/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8793 - loss: 0.3076 - val_accuracy: 0.8834 - val_loss: 0.3097\n",
      "Epoch 30/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8805 - loss: 0.3064 - val_accuracy: 0.8809 - val_loss: 0.3083\n",
      "Epoch 31/100\n",
      "1105/1105 - 11s - 10ms/step - accuracy: 0.8801 - loss: 0.3056 - val_accuracy: 0.8822 - val_loss: 0.3076\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "\n",
      "✅ Model Training Complete.\n",
      "Final Validation Accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the Early Stopping callback\n",
    "# Monitor validation loss and stop if no improvement after 10 epochs\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True, # Keeps the model weights from the best epoch\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_final, \n",
    "    Y_train_final,\n",
    "    validation_data=(X_val_final, Y_val_final),\n",
    "    epochs=100, # Set a high number of epochs; EarlyStopping will intervene\n",
    "    batch_size=64, \n",
    "    callbacks=[early_stopper],\n",
    "    verbose=2 # Shows one line per epoch\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Model Training Complete.\")\n",
    "# Report final validation accuracy\n",
    "final_val_acc = history.history['val_accuracy'][-1] \n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3961ad8c-6370-4e51-bff4-3e0608cb1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input shape: (2000, 967)\n",
      "Number of predictions generated: 2000\n",
      "Positive predictions (1s): 876\n",
      "\n",
      "✅ Submission file 'submission.csv' created successfully.\n",
      "Submission Head:\n",
      "   id  predictions\n",
      "0   1            1\n",
      "1   2            1\n",
      "2   3            0\n",
      "3   4            1\n",
      "4   5            1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Prepare Test Features ---\n",
    "\n",
    "# Exclude all non-numerical/metadata columns, ensuring consistency with training\n",
    "non_nn_cols = ['article_id', 'article_venue', 'candidate', 'contributor', 'participated_years']\n",
    "feature_cols = [col for col in X_train.columns if col not in non_nn_cols] # Use the list from X_train for column order consistency\n",
    "\n",
    "# Align test data features and convert to NumPy array\n",
    "X_test_final = test_df[feature_cols].values\n",
    "print(f\"Test input shape: {X_test_final.shape}\")\n",
    "\n",
    "# --- 2. Generate Predictions ---\n",
    "\n",
    "# Predict probabilities (will be between 0 and 1)\n",
    "probabilities = model.predict(X_test_final, verbose=0)\n",
    "\n",
    "# --- 3. Convert to Binary Predictions ---\n",
    "\n",
    "# Use 0.5 as the threshold for binary classification\n",
    "predictions = np.round(probabilities).astype(int).flatten()\n",
    "\n",
    "print(f\"Number of predictions generated: {len(predictions)}\")\n",
    "print(f\"Positive predictions (1s): {np.sum(predictions)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create Submission File ---\n",
    "\n",
    "# The submission requires the 'id' column from the test_df\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'], # Use the original 'id' column from the test set\n",
    "    'predictions': predictions\n",
    "})\n",
    "\n",
    "# Save the file to CSV format\n",
    "submission_file_name = 'submission.csv'\n",
    "submission_df.to_csv(submission_file_name, index=False)\n",
    "\n",
    "print(f\"\\n✅ Submission file '{submission_file_name}' created successfully.\")\n",
    "print(\"Submission Head:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6692e61-feec-40c8-be0f-bb93ce17a1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
